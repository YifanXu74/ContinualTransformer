{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import random\n",
    "import numpy as np\n",
    "\n",
    "from typing import Any, Callable, Dict, List, NewType, Optional, Tuple, Union\n",
    "\n",
    "import warnings\n",
    "from collections.abc import Mapping\n",
    "\n",
    "from transformers import (\n",
    "    BertTokenizer, BertTokenizerFast\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a={'a':1, 'b':2}\n",
    "c,d=a.values()\n",
    "c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tolist(x):\n",
    "    if isinstance(x, list):\n",
    "        return x\n",
    "    elif hasattr(x, \"numpy\"):  # Checks for TF tensors without needing the import\n",
    "        x = x.numpy()\n",
    "    return x.tolist()\n",
    "\n",
    "class ProcessorForWholeWordMask(torch.nn.Module):\n",
    "    def __init__(self, tokenizer, mlm_probability):\n",
    "        super().__init__()\n",
    "        self.tokenizer = tokenizer\n",
    "        self.mlm_probability = mlm_probability\n",
    "    \n",
    "    def _torch_collate_batch(self, examples):\n",
    "        \"\"\"Collate `examples` into a batch, using the information in `tokenizer` for padding if necessary.\"\"\"\n",
    "        pad_to_multiple_of = None\n",
    "\n",
    "        # Tensorize if necessary.\n",
    "        if isinstance(examples[0], (list, tuple, np.ndarray)):\n",
    "            examples = [torch.tensor(e, dtype=torch.long) for e in examples]\n",
    "\n",
    "        length_of_first = examples[0].size(0)\n",
    "\n",
    "        # Check if padding is necessary.\n",
    "\n",
    "        are_tensors_same_length = all(x.size(0) == length_of_first for x in examples)\n",
    "        if are_tensors_same_length and (pad_to_multiple_of is None or length_of_first % pad_to_multiple_of == 0):\n",
    "            return torch.stack(examples, dim=0)\n",
    "\n",
    "        # If yes, check if we have a `pad_token`.\n",
    "        if self.tokenizer._pad_token is None:\n",
    "            raise ValueError(\n",
    "                \"You are attempting to pad samples but the tokenizer you are using\"\n",
    "                f\" ({self.tokenizer.__class__.__name__}) does not have a pad token.\"\n",
    "            )\n",
    "\n",
    "        # Creating the full tensor and filling it with our data.\n",
    "        max_length = max(x.size(0) for x in examples)\n",
    "        if pad_to_multiple_of is not None and (max_length % pad_to_multiple_of != 0):\n",
    "            max_length = ((max_length // pad_to_multiple_of) + 1) * pad_to_multiple_of\n",
    "        result = examples[0].new_full([len(examples), max_length], self.tokenizer.pad_token_id)\n",
    "        for i, example in enumerate(examples):\n",
    "            if self.tokenizer.padding_side == \"right\":\n",
    "                result[i, : example.shape[0]] = example\n",
    "            else:\n",
    "                result[i, -example.shape[0] :] = example\n",
    "        return result\n",
    "    \n",
    "    def _whole_word_mask(self, input_tokens: List[str], max_predictions=512):\n",
    "        \"\"\"\n",
    "        Get 0/1 labels for masked tokens with whole word mask proxy\n",
    "        \"\"\"\n",
    "        if not isinstance(self.tokenizer, (BertTokenizer, BertTokenizerFast)):\n",
    "            warnings.warn(\n",
    "                \"DataCollatorForWholeWordMask is only suitable for BertTokenizer-like tokenizers. \"\n",
    "                \"Please refer to the documentation for more information.\"\n",
    "            )\n",
    "\n",
    "        cand_indexes = []\n",
    "        for i, token in enumerate(input_tokens):\n",
    "            if token == \"[CLS]\" or token == \"[SEP]\":\n",
    "                continue\n",
    "\n",
    "            if len(cand_indexes) >= 1 and token.startswith(\"##\"):\n",
    "                cand_indexes[-1].append(i)\n",
    "            else:\n",
    "                cand_indexes.append([i])\n",
    "\n",
    "        random.shuffle(cand_indexes)\n",
    "        num_to_predict = min(max_predictions, max(1, int(round(len(input_tokens) * self.mlm_probability))))\n",
    "        masked_lms = []\n",
    "        covered_indexes = set()\n",
    "        for index_set in cand_indexes:\n",
    "            if len(masked_lms) >= num_to_predict:\n",
    "                break\n",
    "            # If adding a whole-word mask would exceed the maximum number of\n",
    "            # predictions, then just skip this candidate.\n",
    "            if len(masked_lms) + len(index_set) > num_to_predict:\n",
    "                continue\n",
    "            is_any_index_covered = False\n",
    "            for index in index_set:\n",
    "                if index in covered_indexes:\n",
    "                    is_any_index_covered = True\n",
    "                    break\n",
    "            if is_any_index_covered:\n",
    "                continue\n",
    "            for index in index_set:\n",
    "                covered_indexes.add(index)\n",
    "                masked_lms.append(index)\n",
    "\n",
    "        if len(covered_indexes) != len(masked_lms):\n",
    "            raise ValueError(\"Length of covered_indexes is not equal to length of masked_lms.\")\n",
    "        mask_labels = [1 if i in covered_indexes else 0 for i in range(len(input_tokens))]\n",
    "        return mask_labels\n",
    "    \n",
    "    def torch_mask_tokens(self, inputs: Any, mask_labels: Any) -> Tuple[Any, Any]:\n",
    "        \"\"\"\n",
    "        Prepare masked tokens inputs/labels for masked language modeling: 80% MASK, 10% random, 10% original. Set\n",
    "        'mask_labels' means we use whole word mask (wwm), we directly mask idxs according to it's ref.\n",
    "        \"\"\"\n",
    "        import torch\n",
    "\n",
    "        if self.tokenizer.mask_token is None:\n",
    "            raise ValueError(\n",
    "                \"This tokenizer does not have a mask token which is necessary for masked language modeling. Remove the\"\n",
    "                \" --mlm flag if you want to use this tokenizer.\"\n",
    "            )\n",
    "        labels = inputs.clone()\n",
    "        # We sample a few tokens in each sequence for masked-LM training (with probability args.mlm_probability defaults to 0.15 in Bert/RoBERTa)\n",
    "\n",
    "        probability_matrix = mask_labels\n",
    "\n",
    "        special_tokens_mask = [\n",
    "            self.tokenizer.get_special_tokens_mask(val, already_has_special_tokens=True) for val in labels.tolist()\n",
    "        ]\n",
    "        probability_matrix.masked_fill_(torch.tensor(special_tokens_mask, dtype=torch.bool), value=0.0)\n",
    "        if self.tokenizer._pad_token is not None:\n",
    "            padding_mask = labels.eq(self.tokenizer.pad_token_id)\n",
    "            probability_matrix.masked_fill_(padding_mask, value=0.0)\n",
    "\n",
    "        masked_indices = probability_matrix.bool()\n",
    "        labels[~masked_indices] = -100  # We only compute loss on masked tokens\n",
    "\n",
    "        # 80% of the time, we replace masked input tokens with tokenizer.mask_token ([MASK])\n",
    "        indices_replaced = torch.bernoulli(torch.full(labels.shape, 0.8)).bool() & masked_indices\n",
    "        inputs[indices_replaced] = self.tokenizer.convert_tokens_to_ids(self.tokenizer.mask_token)\n",
    "\n",
    "        # 10% of the time, we replace masked input tokens with random word\n",
    "        indices_random = torch.bernoulli(torch.full(labels.shape, 0.5)).bool() & masked_indices & ~indices_replaced\n",
    "        random_words = torch.randint(len(self.tokenizer), labels.shape, dtype=torch.long)\n",
    "        inputs[indices_random] = random_words[indices_random]\n",
    "\n",
    "        # The rest of the time (10% of the time) we keep the masked input tokens unchanged\n",
    "        return inputs, labels\n",
    "    \n",
    "    def forward(self, examples):\n",
    "        if isinstance(examples, Mapping):\n",
    "            input_ids = examples[\"input_ids\"]\n",
    "            examples = [{\"input_ids\": e} for e in input_ids]\n",
    "        elif isinstance(examples[0], Mapping):\n",
    "            input_ids = [e[\"input_ids\"] for e in examples]\n",
    "        else:\n",
    "            input_ids = examples\n",
    "            examples = [{\"input_ids\": e} for e in examples]\n",
    "\n",
    "        batch_input = self._torch_collate_batch(input_ids)\n",
    "    \n",
    "        mask_labels = []\n",
    "        for e in examples:\n",
    "            ref_tokens = []\n",
    "            for id in tolist(e[\"input_ids\"]):\n",
    "                token = self.tokenizer._convert_id_to_token(id)\n",
    "                ref_tokens.append(token)\n",
    "\n",
    "            # For Chinese tokens, we need extra inf to mark sub-word, e.g [喜,欢]-> [喜，##欢]\n",
    "            if \"chinese_ref\" in e:\n",
    "                ref_pos = tolist(e[\"chinese_ref\"])\n",
    "                len_seq = len(e[\"input_ids\"])\n",
    "                for i in range(len_seq):\n",
    "                    if i in ref_pos:\n",
    "                        ref_tokens[i] = \"##\" + ref_tokens[i]\n",
    "            mask_labels.append(self._whole_word_mask(ref_tokens))\n",
    "        \n",
    "        batch_mask = self._torch_collate_batch(mask_labels)\n",
    "        inputs, labels = self.torch_mask_tokens(batch_input, batch_mask)\n",
    "\n",
    "        return {\"input_ids\": inputs, \"labels\": labels}\n",
    "        \n",
    "\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_pretrained_tokenizer(from_pretrained):\n",
    "    if torch.distributed.is_initialized():\n",
    "        if torch.distributed.get_rank() == 0:\n",
    "            BertTokenizer.from_pretrained(\n",
    "                from_pretrained, do_lower_case=\"uncased\" in from_pretrained\n",
    "            )\n",
    "        torch.distributed.barrier()\n",
    "    return BertTokenizer.from_pretrained(\n",
    "        from_pretrained, do_lower_case=\"uncased\" in from_pretrained\n",
    "    )\n",
    "tokenizer = get_pretrained_tokenizer(\"bert-base-uncased\")\n",
    "mlm_processor = ProcessorForWholeWordMask(tokenizer, mlm_probability=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "caption1 = 'I am so cool!'\n",
    "encoding1 = tokenizer(\n",
    "            caption1,\n",
    "            padding=\"max_length\",\n",
    "            truncation=True,\n",
    "            max_length=40,\n",
    "            return_special_tokens_mask=True,\n",
    "        )\n",
    "caption2 = 'That giraffe'\n",
    "encoding2 = tokenizer(\n",
    "            caption2,\n",
    "            padding=\"max_length\",\n",
    "            truncation=True,\n",
    "            max_length=40,\n",
    "            return_special_tokens_mask=True,\n",
    "        )\n",
    "examples = [encoding1, encoding2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "caption1 = ['I am so cool!', 'That giraffe']\n",
    "examples = tokenizer(\n",
    "            caption1,\n",
    "            padding=\"max_length\",\n",
    "            truncation=True,\n",
    "            max_length=40,\n",
    "            return_special_tokens_mask=True,\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ckpt_path = 'checkpoints/mae_pretrain_vit_base.pth'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ckpt = torch.load(ckpt_path, map_location='cpu')\n",
    "state_dict = ckpt['model']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/yfxu/miniconda3/envs/torch/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from models_cook import ContinualModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "def get_args_parser():\n",
    "    parser = argparse.ArgumentParser('ContinualTransformer pre-training', add_help=False)\n",
    "    # architecture\n",
    "    parser.add_argument('--image_size', default=224, type=int)\n",
    "    parser.add_argument('--model', default='vlmo_base_patch16', type=str)\n",
    "    parser.add_argument('--drop_path_rate', default=0.1, type=float)\n",
    "    # language modeling\n",
    "    parser.add_argument('--max_text_len', default=196, type=int)\n",
    "    parser.add_argument('--max_text_len_of_initckpt', default=196, type=int)\n",
    "    parser.add_argument('--vocab_size', default=30522, type=int)\n",
    "    parser.add_argument('--mlm_probability', default=0.15, type=float)\n",
    "    return parser\n",
    "config = get_args_parser().parse_known_args()[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "window_size: (14, 14)\n",
      "/home/yfxu/miniconda3/envs/torch/lib/python3.10/site-packages/torch/functional.py:504: UserWarning: torch.meshgrid: in an upcoming release, it will be required to pass the indexing argument. (Triggered internally at ../aten/src/ATen/native/TensorShape.cpp:3483.)\n",
      "  return _VF.meshgrid(tensors, **kwargs)  # type: ignore[attr-defined]\n"
     ]
    }
   ],
   "source": [
    "# config={'image_size': 224, 'model': 'vlmo_base_patch16', 'drop_path_rate': 0.1, 'max_text_len': 196, 'vocab_size':30522, 'max_text_len_of_initckpt': 196, 'mlm_probability': 0.15}\n",
    "m = ContinualModel(config=config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from util.misc import convert_init_ckpt\n",
    "\n",
    "a=convert_init_ckpt('checkpoints/beit_base_patch16_224_pt22k_ft22kto1k.pth', module=m, config=config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save({\"model\": a}, 'checkpoints/beit_base_patch16_224_pt22k_ft22kto1k_transfertovlmo.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'ContinualModel' object has no attribute 'mlm_loss'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[39mgetattr\u001b[39;49m(m, \u001b[39m\"\u001b[39;49m\u001b[39mmlm_loss\u001b[39;49m\u001b[39m\"\u001b[39;49m)\n",
      "File \u001b[0;32m~/miniconda3/envs/torch/lib/python3.10/site-packages/torch/nn/modules/module.py:1614\u001b[0m, in \u001b[0;36mModule.__getattr__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m   1612\u001b[0m     \u001b[39mif\u001b[39;00m name \u001b[39min\u001b[39;00m modules:\n\u001b[1;32m   1613\u001b[0m         \u001b[39mreturn\u001b[39;00m modules[name]\n\u001b[0;32m-> 1614\u001b[0m \u001b[39mraise\u001b[39;00m \u001b[39mAttributeError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39m'\u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m'\u001b[39m\u001b[39m object has no attribute \u001b[39m\u001b[39m'\u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m'\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39mformat(\n\u001b[1;32m   1615\u001b[0m     \u001b[39mtype\u001b[39m(\u001b[39mself\u001b[39m)\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m, name))\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'ContinualModel' object has no attribute 'mlm_loss'"
     ]
    }
   ],
   "source": [
    "]]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
